diff --git a/python/triton/runtime/autotuner.py b/python/triton/runtime/autotuner.py
index c9833c94..cb4e4a99 100644
--- a/python/triton/runtime/autotuner.py
+++ b/python/triton/runtime/autotuner.py
@@ -145,10 +145,19 @@ class Autotuner(KernelInterface):
         current = dict(meta, **config.all_kwargs())
         full_nargs = {**self.nargs, **current}
 
+        # Add shape postfix to the generated directory
+        # FIXME:
+        TUNING_SHAPE_CONFIG=""
+        # TUNING_SHAPE_CONFIG=os.getenv("ENABLE_AUTOTUNING")
+        for k in config.kwargs.keys():
+          TUNING_SHAPE_CONFIG += "_" + str(config.all_kwargs()[k])
+        os.environ["TUNING_SHAPE_CONFIG"] = TUNING_SHAPE_CONFIG
+        # print("set TUNING_SHAPE_CONFIG :", TUNING_SHAPE_CONFIG)
+
         def kernel_call():
-            if config.pre_hook:
-                config.pre_hook(full_nargs)
-            self.pre_hook(full_nargs)
+            # if config.pre_hook:
+            #     config.pre_hook(full_nargs)
+            # self.pre_hook(full_nargs)
             try:
                 self.fn.run(
                     *args,
@@ -161,10 +170,12 @@ class Autotuner(KernelInterface):
                     # Throw exception raised by `self.fn.run`
                     raise
 
-            self.post_hook(full_nargs, exception=None)
+            # self.post_hook(full_nargs, exception=None)
 
         try:
-            return self.do_bench(kernel_call, quantiles=(0.5, 0.2, 0.8))
+            # return self.do_bench(kernel_call, quantiles=(0.5, 0.2, 0.8))
+            kernel_call()
+            return [float("inf"), float("inf"), float("inf")]
         except (OutOfResources, CompileTimeAssertionFailure, PTXASError) as e:
             if verbose:
                 print(f"Autotuning failed with {e}")
@@ -196,20 +207,21 @@ class Autotuner(KernelInterface):
             config = self.cache[key]
         else:
             config = self.configs[0]
-        self.best_config = config
-        if os.getenv("TRITON_PRINT_AUTOTUNING", None) == "1" and not used_cached_result:
-            print(f"Triton autotuning for function {self.base_fn.__name__} finished after "
-                  f"{self.bench_time:.2f}s; best config selected: {self.best_config};")
-        if config.pre_hook is not None:
-            full_nargs = {**self.nargs, **kwargs, **config.all_kwargs()}
-            config.pre_hook(full_nargs)
+        self.best_config = config
+        if os.getenv("TRITON_PRINT_AUTOTUNING", None) == "1" and not used_cached_result:
+            print(f"Triton autotuning for function {self.base_fn.__name__} finished after "
+                  f"{self.bench_time:.2f}s; best config selected: {self.best_config};")
+        # if config.pre_hook is not None:
+        #     full_nargs = {**self.nargs, **kwargs, **config.all_kwargs()}
+        #     config.pre_hook(full_nargs)
+        os.environ.pop('TUNING_SHAPE_CONFIG', None)
         ret = self.fn.run(
             *args,
             **kwargs,
             **config.all_kwargs(),
         )
         self.nargs = None
-        return ret
+        # return ret
 
     def prune_configs(self, kwargs):
         pruned_configs = self.configs
diff --git a/python/triton/runtime/jit.py b/python/triton/runtime/jit.py
index 1c7722ee..5dbc12dc 100644
--- a/python/triton/runtime/jit.py
+++ b/python/triton/runtime/jit.py
@@ -635,7 +635,13 @@ class JITFunction(KernelInterface[T]):
             self.cache[device_key][key] = kernel
 
         launcher_src_dir = os.getenv("KERNEL_AUX_FILE_DIR")
+        block_shape = os.getenv("TUNING_SHAPE_CONFIG")
+        # print("get TUNING_SHAPE_CONFIG :", block_shape)
+        if block_shape is None:
+            block_shape = ""
+
         if launcher_src_dir is not None:
+            launcher_src_dir += block_shape
             os.makedirs(launcher_src_dir, mode=0o777, exist_ok=True)
             ttcir_path = os.path.join(launcher_src_dir, kernel.name + ".ttcir")
             tttcir_path = os.path.join(launcher_src_dir, kernel.name + ".tttcir")
@@ -648,7 +654,6 @@ class JITFunction(KernelInterface[T]):
                 f.write(kernel.asm["llir"])
 
 
-
         # Check that used global values have not changed.
         not_present = object()
         for (name, _), (val, globals_dict) in self.used_global_vals.items():
diff --git a/third_party/cpu/backend/driver.py b/third_party/cpu/backend/driver.py
index 982540e9..21ff4c93 100644
--- a/third_party/cpu/backend/driver.py
+++ b/third_party/cpu/backend/driver.py
@@ -57,14 +57,20 @@ if os.path.exists(sys_lib_dir):
 def compile_module_from_src(inc, src, kernel_name):
     launcher_include_dir = os.getenv("KERNEL_LAUNCHER_INCLUDE_DIR")
     launcher_src_dir = os.getenv("KERNEL_AUX_FILE_DIR")
+
+    block_shape = os.getenv("TUNING_SHAPE_CONFIG")
+    if block_shape is None:
+        block_shape =""
+
     if launcher_include_dir is None:
        launcher_include_dir = tempfile.mkdtemp()
 
-    os.makedirs(launcher_include_dir, mode=0o777, exist_ok=True)
 
     if launcher_src_dir is None:
        launcher_src_dir = launcher_include_dir
 
+    launcher_src_dir += block_shape
+    os.makedirs(launcher_include_dir, mode=0o777, exist_ok=True)
     os.makedirs(launcher_src_dir, mode=0o777, exist_ok=True)
 
 
@@ -135,7 +141,7 @@ def ty_to_cpp(ty):
     }[ty]
 
 
-def make_launcher(constants, signature,ids, kernel_name):
+def make_launcher(constants, signature, ids, kernel_name, constexprs_arg_names):
     # Record the end of regular arguments;
     # subsequent arguments are architecture-specific descriptors.
     arg_decls = ', '.join(f"{ty_to_cpp(ty)} arg{i}" for i, ty in signature.items())
@@ -144,6 +150,11 @@ def make_launcher(constants, signature,ids, kernel_name):
     kernel_fn_args_list = ', '.join(f"arg{i}" for i in kernel_fn_args)
     kernel_fn_arg_types = ', '.join([f"{ty_to_cpp(signature[i])}" for i in kernel_fn_args] + ["uint32_t"] * 6)
 
+    kernel_constants_declare = "".join(f"extern const int {kernel_name}_{arg_name};\n" for arg_id, arg_name in constexprs_arg_names.items() if isinstance(constants[arg_id], int) )
+    kernel_constants_definition = "".join(f"const int {kernel_name}_{arg_name} = {constants[arg_id]};\n" for arg_id, arg_name in constexprs_arg_names.items() if isinstance(constants[arg_id], int))
+    # print(kernel_constants_declare)
+    # print(kernel_constants_definition)
+
     inc=f"""
 #include <cstddef>
 #include <stdint.h>
@@ -155,6 +166,8 @@ extern "C"{{
  void({kernel_name})({kernel_fn_arg_types});
  }}
 
+{kernel_constants_declare}
+
 void {kernel_name}_omp(uint32_t gridX, uint32_t gridY, uint32_t gridZ,
                         {kernel_name}_kernel_ptr_t kernel_ptr {', ' + arg_decls if len(arg_decls) > 0 else ''});
 """
@@ -166,6 +179,9 @@ void {kernel_name}_omp(uint32_t gridX, uint32_t gridY, uint32_t gridZ,
 #include <algorithm>
 #include <optional>
 #include <stdio.h>
+
+{kernel_constants_definition}
+
 void {kernel_name}_omp(uint32_t gridX, uint32_t gridY, uint32_t gridZ, {kernel_name}_kernel_ptr_t kernel_ptr {', ' + arg_decls if len(arg_decls) > 0 else ''}) {{
   // TODO: Consider using omp collapse(3) clause for simplicity?
   size_t N = gridX * gridY * gridZ;
@@ -213,9 +229,12 @@ class CPULauncher(object):
         ids = {"ids_of_const_exprs": src.fn.constexprs if hasattr(src, "fn") else tuple()}
         constants = src.constants if hasattr(src, "constants") else dict()
         cst_key = lambda i: src.fn.arg_names.index(i) if isinstance(i, str) else i
+
+        constexprs_arg_names = {cst_key(key): key for key, value in constants.items()  if(cst_key(key) in  src.fn.constexprs)}
+
         constants = {cst_key(key): value for key, value in constants.items()}
         signature = {cst_key(key): value for key, value in src.signature.items()}
-        inc, src = make_launcher(constants, signature, ids, name)
+        inc, src = make_launcher(constants, signature, ids, name, constexprs_arg_names)
         compile_module_from_src(inc, src, name)
         # self.launch = mod.launch
 
